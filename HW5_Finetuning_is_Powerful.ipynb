{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check GPU Availability",
   "id": "a46c2289819f2b43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:42:03.260941Z",
     "start_time": "2025-04-06T15:42:02.483169Z"
    }
   },
   "cell_type": "code",
   "source": "!nvidia-smi",
   "id": "9650fb7e7a6b6431",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Sun Apr  6 15:42:02 2025       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:25:00.0 Off |                    0 |\r\n",
      "| N/A   23C    P0             36W /  400W |       1MiB /  81920MiB |      0%      Default |\r\n",
      "|                                         |                        |             Disabled |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Installation",
   "id": "643214d111195205"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use this docker image: pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel\n",
    "# Temporarily as of Jan 31st 2025, Colab has some issues with Pytorch\n",
    "# Using pip install unsloth will take 3 minutes, whilst the below takes <1 minute:\n",
    "# !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "# !pip install --no-deps cut_cross_entropy\n",
    "# !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "# !pip install unsloth==2025.2.15 unsloth_zoo==2025.2.7\n",
    "# !pip install transformers==4.49.0"
   ],
   "id": "763b661ccaa5235a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !git clone https://github.com/ericsunkuan/ML_Spring2025_HW5.git",
   "id": "ff3e0f6a52e1adb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Unsloth",
   "id": "de0b34841dfb5b02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Note : Changing the model is against the Rules of Homework 5 !!!",
   "id": "50a25afe540fd31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initialize the LLM",
   "id": "5fe722458c0d64bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:55:41.586483Z",
     "start_time": "2025-04-06T15:54:29.181949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "### Changing the model here is forbidden !\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-2-7b-bnb-4bit\",  ### Do not change the model for any other models or quantization versions\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ],
   "id": "489a01c5e6a4063c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.138 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Add LoRA adapters so we only need to update 1 to 10% of all parameters!",
   "id": "d3391b3282f1293f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:55:52.060850Z",
     "start_time": "2025-04-06T15:55:47.958384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "################# TODO : Tweak the LoRA adapter hyperparameters here.  #####################\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,\n",
    "    ### TODO : Choose any number > 0 ! Common values are 4, 8, 16, 32, 64, 128. Higher ranks allow more expressive power but also increase parameter count.\n",
    "    lora_alpha=16,  ### TODO : Choose any number > 0 ! Suggested 4, 8, 16, 32, 64, 128\n",
    "\n",
    "    ################# TODO  ####################################################################\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\", ],\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # We support rank stabilized LoRA\n",
    "    loftq_config=None,  # And LoftQ\n",
    ")"
   ],
   "id": "ef532e06caa79705",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:55:53.319358Z",
     "start_time": "2025-04-06T15:55:53.308503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3.1\",  ### Use llama-3.1 template for better performance here\n",
    ")\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "    return {\"text\": texts, }"
   ],
   "id": "f0e0f53f6470bbd0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset Preperation (Loading and Refining)",
   "id": "dbc3b1c25eed263c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Filtering & Sorting",
   "id": "c103afc8ca48725f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:56:03.178477Z",
     "start_time": "2025-04-06T15:55:56.030787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_from_disk(\"dataset/ml2025-hw5/fastchat_alpaca_52k\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Add a \"text\" field to each example\n",
    "# ---------------------------\n",
    "# This function extracts the first assistant message from the conversation\n",
    "def add_text_field(example):\n",
    "    # Extract the first message where role == 'assistant'\n",
    "    assistant_texts = [msg[\"content\"] for msg in example[\"conversations\"] if msg[\"role\"] == \"assistant\"]\n",
    "    text = assistant_texts[0] if assistant_texts else \"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "# Map the function over the dataset to add the \"text\" column.\n",
    "dataset = dataset.map(add_text_field)\n",
    "\n",
    "# Print the dataset structure to confirm the new feature.\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "#################### TODO : Define a helper function for computing conversation length ###############\n",
    "# The default \"conversation length\" here refers to the length of the input (human) and output (gpt), you can modify it at your will\n",
    "def compute_conversation_length(example):\n",
    "    # Compute total word count across all messages in the 'conversations' field\n",
    "    return sum(len(message[\"content\"].split()) for message in example[\"conversations\"])\n",
    "\n",
    "\n",
    "#################### TODO ############################################################################\n",
    "# ---------------------------\n",
    "# Simple Sorting Method  (Default)\n",
    "# ---------------------------\n",
    "# Sort the dataset from longest to shortest conversation (by word count)\n",
    "sorted_dataset_simple_list = sorted(dataset, key=compute_conversation_length, reverse=True)\n",
    "\n",
    "# Convert back to a Dataset object\n",
    "sorted_dataset_simple = Dataset.from_list(sorted_dataset_simple_list)\n",
    "\n",
    "print(\"\\nTop examples sorted by simple conversation length:\")\n",
    "for entry in sorted_dataset_simple.select(range(5)):\n",
    "    print(f\"ID: {entry['id']}, Conversation Length: {compute_conversation_length(entry)}\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "############## Advanced Sorting Method (TODO : Modify the sorting key ##################\n",
    "# ---------------------------\n",
    "# Default : Sorting based on Combining conversation length with the 'score' field using a weighted sum.\n",
    "# Here, we multiply the score by 10 and add it to the conversation length.\n",
    "def advanced_sort_key(example):\n",
    "    conversation_len = compute_conversation_length(example)\n",
    "    score = example[\"score\"]\n",
    "    return 1e-5 * conversation_len + score * 1\n",
    "\n",
    "\n",
    "####################################### TODO ###########################################\n",
    "sorted_dataset_advanced_list = sorted(dataset, key=advanced_sort_key, reverse=True)\n",
    "# Convert back to a Dataset object\n",
    "sorted_dataset_advanced = Dataset.from_list(sorted_dataset_advanced_list)\n",
    "\n",
    "print(\"\\nTop examples sorted by advanced key (combination of conversation length and score):\")\n",
    "for entry in sorted_dataset_advanced.select(range(5)):\n",
    "    print(f\"ID: {entry['id']}, Advanced Key Value: {advanced_sort_key(entry)}\")"
   ],
   "id": "587b6ecf37be0495",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'conversations', 'score', 'text'],\n",
      "    num_rows: 52002\n",
      "})\n",
      "\n",
      "Top examples sorted by simple conversation length:\n",
      "ID: identity_45289, Conversation Length: 759\n",
      "ID: identity_6285, Conversation Length: 670\n",
      "ID: identity_15102, Conversation Length: 622\n",
      "ID: identity_18853, Conversation Length: 567\n",
      "ID: identity_15908, Conversation Length: 558\n",
      "\n",
      "Top examples sorted by advanced key (combination of conversation length and score):\n",
      "ID: identity_23370, Advanced Key Value: 5.00049\n",
      "ID: identity_37225, Advanced Key Value: 5.00047\n",
      "ID: identity_40362, Advanced Key Value: 5.0004\n",
      "ID: identity_8015, Advanced Key Value: 5.00037\n",
      "ID: identity_30813, Advanced Key Value: 5.00037\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Note : You are limited to use 100 sorted data among the 1000 data in the given dataset, no more than 100 data is allowed for training !!!",
   "id": "cda848724ac82727"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:56:05.474323Z",
     "start_time": "2025-04-06T15:56:05.378459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "################# TODO : select the simple or advanced dataset for training ##############\n",
    "\n",
    "dataset_used = \"sorted_dataset_advanced\"  #sorted_dataset_advanced\n",
    "\n",
    "################# TODO ###################################################################\n",
    "\n",
    "if dataset_used == \"sorted_dataset_simple\":\n",
    "    train_dataset = sorted_dataset_simple.select(\n",
    "        range(0, 100))  ### You can also select from the middle, e.g. sorted_dataset_simple.select(range(50,150))\n",
    "else:\n",
    "    train_dataset = sorted_dataset_advanced.select(range(0, 100))\n",
    "\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "train_dataset = standardize_sharegpt(train_dataset)\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)"
   ],
   "id": "c94489f5de418fe9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Standardizing format:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "346da092feb9404189d06541f33df19c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15a3ef05fac94c149ba9ea3f19f4562e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset Visualize",
   "id": "6b1558f79b0cc508"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:56:10.489604Z",
     "start_time": "2025-04-06T15:56:10.480379Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset[0][\"conversations\"]",
   "id": "22738290f81483be",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: Rewrite the sentence below into a clear and concise instruction ### Input: Please find a way to format the document',\n",
       "  'role': 'user'},\n",
       " {'content': 'Format the document.', 'role': 'assistant'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And we see how the chat template transformed these conversations.\n",
    "\n",
    "**[Notice]** Llama 3.1 Instruct's default chat template default adds `\"Cutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\"`, so do not be alarmed!"
   ],
   "id": "530cf6c6c1de9032"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:56:12.722733Z",
     "start_time": "2025-04-06T15:56:12.715752Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_dataset[0][\"text\"])",
   "id": "b16de0bbcb9cbc79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: Rewrite the sentence below into a clear and concise instruction ### Input: Please find a way to format the document<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Format the document.<|eot_id|>\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "61a7970c0034bb93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:56:18.279602Z",
     "start_time": "2025-04-06T15:56:15.957596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "################# TODO : Tweak the training hyperparameters here.  #####################\n",
    "training_config = {\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 10,\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"seed\": 3407,  ### Do not modify the seed for reproducibility\n",
    "}\n",
    "\n",
    "################# TODO #################################################################\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=training_config[\"per_device_train_batch_size\"],\n",
    "        gradient_accumulation_steps=training_config[\"gradient_accumulation_steps\"],\n",
    "        warmup_steps=training_config[\"warmup_steps\"],\n",
    "        num_train_epochs=training_config[\"num_train_epochs\"],  # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate=training_config[\"learning_rate\"],\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=training_config[\"optim\"],\n",
    "        weight_decay=training_config[\"weight_decay\"],\n",
    "        lr_scheduler_type=training_config[\"lr_scheduler_type\"],\n",
    "        seed=training_config[\"seed\"],\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",  # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ],
   "id": "b184ea70953255cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Converting train dataset to ChatML (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c84d0707cb18410aadf87a81afda2c40"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Applying chat template to train dataset (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba68e13e5f7c4d25a0518cca3c1b1445"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "581361f161d74d5a9fb31d6001e920c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Truncating train dataset (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f36c13dc70654cb8b77fac6b2eca53ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs.",
   "id": "6d97dd6f5ea26b93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:56:23.057385Z",
     "start_time": "2025-04-06T15:56:22.914475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ],
   "id": "4313636a4a96fd69",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ea9c9654a674c308d64a51d24a40ded"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer_stats = trainer.train()",
   "id": "bedfb1faba2b79"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### TODO : Curriculum Training  (Optional)\n",
    "start training the LLM with “easier” examples (e.g., shorter, clearer conversations) and progressively introduce more complex ones.\n",
    "\n",
    "The total data amount used to train should still not exceed 100 data."
   ],
   "id": "7ac2adad5c104291"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "############## TODO : Curriculum Training  ######################\n",
    "\n",
    "### E.g.\n",
    "### Step 1. Train on sorted_dataset_simple\n",
    "### Step 2. Train on sorted_dataset_advanced"
   ],
   "id": "6dfd6b09e4ec9f9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"Inference\"></a>\n",
    "## Inference\n"
   ],
   "id": "c0049fe6b81d995c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def parse_true_output(text):\n",
    "    \"\"\"\n",
    "    Extracts the true assistant output from the decoded model output.\n",
    "\n",
    "    It looks for the assistant header token:\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    and extracts everything after it until the first occurrence of \"<|eot_id|>\".\n",
    "    If the assistant header is not found, it falls back to the last occurrence\n",
    "    of \"<|end_header_id|>\\n\\n\". If \"<|eot_id|>\" is not found, the extraction\n",
    "    continues until the end of the string.\n",
    "    \"\"\"\n",
    "    assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    start_index = text.find(assistant_header)\n",
    "    if start_index != -1:\n",
    "        start_index += len(assistant_header)\n",
    "    else:\n",
    "        # Fallback: use the last occurrence of the generic header ending\n",
    "        generic_header = \"<|end_header_id|>\\n\\n\"\n",
    "        start_index = text.rfind(generic_header)\n",
    "        if start_index != -1:\n",
    "            start_index += len(generic_header)\n",
    "        else:\n",
    "            start_index = 0\n",
    "\n",
    "    end_index = text.find(\"<|eot_id|>\", start_index)\n",
    "    if end_index == -1:\n",
    "        end_index = len(text)\n",
    "    return text[start_index:end_index].strip()"
   ],
   "id": "e11558e2ba6fc895"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "# Load the test set JSON file (without GPT responses)\n",
    "with open(\"dataset/ml2025-hw5/test_set_evol_instruct_150.json\", \"r\") as infile:\n",
    "    test_data = json.load(infile)\n",
    "\n",
    "# Dictionary to store inference results\n",
    "inference_results = {}\n",
    "\n",
    "# Loop over each data entry in the test set\n",
    "for index, entry in enumerate(test_data):\n",
    "    entry_id = entry.get(\"id\", \"unknown_id\")\n",
    "\n",
    "    # Build the messages list from the human conversation entries\n",
    "    # (Test set is expected to have only \"human\" messages)\n",
    "    messages = []\n",
    "    for conv in entry.get(\"conversations\", []):\n",
    "        if conv.get(\"from\") == \"human\":\n",
    "            messages.append({\"role\": \"user\", \"content\": conv.get(\"value\", \"\")})\n",
    "        else:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": conv.get(\"value\", \"\")})\n",
    "\n",
    "    # Create inputs using the chat template (required for generation)\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,  # Must add for generation\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    ################# TODO : Tweak Decoding Parameters here.  #####################\n",
    "\n",
    "    # Generate model outputs\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=100,\n",
    "        use_cache=True,\n",
    "        temperature=1.5,\n",
    "        top_p=0.9,\n",
    "        top_k=30,\n",
    "    )\n",
    "\n",
    "    ################# TODO  ##########################################################\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "    # Parse each output to extract the true assistant response\n",
    "    parsed_outputs = [parse_true_output(output) for output in decoded_outputs]\n",
    "\n",
    "    # Store the result for the current entry\n",
    "    inference_results[entry_id] = {\n",
    "        \"input\": messages,\n",
    "        \"output\": parsed_outputs\n",
    "    }\n",
    "\n",
    "    print(f\"Inference completed for entry {entry_id}\")\n",
    "\n",
    "#Write the inference results to the prediction JSON file\n",
    "with open(f\"pred.json\", \"w\") as outfile:\n",
    "    json.dump(inference_results, outfile, indent=4)\n",
    "with open(f\"training_config.json\", \"w\") as outfile:\n",
    "    json.dump(training_config, outfile, indent=4)\n",
    "\n",
    "print(\"Inference completed for all entries in the test set.\")"
   ],
   "id": "150560f8035b44d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Saving, loading finetuned models",
   "id": "e41b0b466a54c543"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save the model",
   "id": "3fc0ddfc1de688d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ],
   "id": "e4fdf0e5a57cbaab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load the model",
   "id": "f47b58c175e91a51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"lora_model\",\n",
    "    # The folder path containing of the folder that contains adapter_model.safetensors, adapter_config.json and README.md\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference"
   ],
   "id": "3f37a58b2bb59ef6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compare the performance of the finetuned model with the original model",
   "id": "abd9e37adf61a93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the finetuned model",
   "id": "da00ef25461ce2ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:46:38.978734Z",
     "start_time": "2025-04-06T15:45:33.380008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"lora_model\",\n",
    "    # The folder path containing of the folder that contains adapter_model.safetensors, adapter_config.json and README.md\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3.1\",\n",
    ")"
   ],
   "id": "aeba43def6a70362",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.138 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:47:03.894074Z",
     "start_time": "2025-04-06T15:46:49.857576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"你好，请用不超过200字的内容介绍一下什么是人工智能。\"\n",
    "messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=500,\n",
    "    use_cache=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=30,\n",
    ")\n",
    "decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "print(decoded_outputs[0])"
   ],
   "id": "4607545e4ec60e90",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "你好，请用不超过200字的内容介绍一下什么是人工智能。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "人工智能（Artificial Intelligence, AI）是指通过计算机来模拟人类的智能。通过大量的数据训练，人工智能可以自动学习并运行，以获得与人类一致的智能。人工智能可以做到各种复杂的任务，比如图片分类、语音辨识、游戏玩家的行为分析等。人工智能可以在实时的敏捷性、高准确度、高效率、可靠性和自我修复等方面比人类更好。人工智能可以在医疗、教育、科学研究、评论、营销、军事、工程设计等领域使用。人工智能可以改善工作效率，同时帮助人类探索更多的可能性。人工智能的应用范围不断扩大，将带来更多的机器人化、混合现实等新的技术。</s>\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the original model",
   "id": "deaa192d303dc4a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:52:28.156511Z",
     "start_time": "2025-04-06T15:51:28.592987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-2-7b-bnb-4bit\",  ### Do not change the model for any other models or quantization versions\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3.1\",\n",
    ")"
   ],
   "id": "9b3e76a2cd8e2bb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.138 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:53:41.972799Z",
     "start_time": "2025-04-06T15:53:38.958274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"你好，请用不超过200字的内容介绍一下什么是人工智能。\"\n",
    "messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=500,\n",
    "    use_cache=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=30,\n",
    ")\n",
    "decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "print(decoded_outputs[0])"
   ],
   "id": "68e096b00a0e00cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "你好，请用不超过200字的内容介绍一下什么是人工智能。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "感谢你的反馈。\n",
      "\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|eot_id|><|start_header_id|>system<|end_header_id|>\n",
      "</s>\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
