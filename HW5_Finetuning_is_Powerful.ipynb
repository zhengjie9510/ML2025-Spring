{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check GPU Availability",
   "id": "a46c2289819f2b43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:42:03.260941Z",
     "start_time": "2025-04-06T15:42:02.483169Z"
    }
   },
   "cell_type": "code",
   "source": "!nvidia-smi",
   "id": "9650fb7e7a6b6431",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Sun Apr  6 15:42:02 2025       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:25:00.0 Off |                    0 |\r\n",
      "| N/A   23C    P0             36W /  400W |       1MiB /  81920MiB |      0%      Default |\r\n",
      "|                                         |                        |             Disabled |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|  No running processes found                                                             |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Installation",
   "id": "643214d111195205"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use this docker image: pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel\n",
    "# Temporarily as of Jan 31st 2025, Colab has some issues with Pytorch\n",
    "# Using pip install unsloth will take 3 minutes, whilst the below takes <1 minute:\n",
    "# !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "# !pip install --no-deps cut_cross_entropy\n",
    "# !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "# !pip install unsloth==2025.2.15 unsloth_zoo==2025.2.7\n",
    "# !pip install transformers==4.49.0"
   ],
   "id": "763b661ccaa5235a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !git clone https://github.com/ericsunkuan/ML_Spring2025_HW5.git",
   "id": "ff3e0f6a52e1adb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Unsloth",
   "id": "de0b34841dfb5b02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Note : Changing the model is against the Rules of Homework 5 !!!",
   "id": "50a25afe540fd31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initialize the LLM",
   "id": "5fe722458c0d64bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:55:41.586483Z",
     "start_time": "2025-04-06T15:54:29.181949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "### Changing the model here is forbidden !\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-2-7b-bnb-4bit\",  ### Do not change the model for any other models or quantization versions\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ],
   "id": "489a01c5e6a4063c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.138 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Add LoRA adapters so we only need to update 1 to 10% of all parameters!",
   "id": "d3391b3282f1293f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:55:52.060850Z",
     "start_time": "2025-04-06T15:55:47.958384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "################# TODO : Tweak the LoRA adapter hyperparameters here.  #####################\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,\n",
    "    ### TODO : Choose any number > 0 ! Common values are 4, 8, 16, 32, 64, 128. Higher ranks allow more expressive power but also increase parameter count.\n",
    "    lora_alpha=16,  ### TODO : Choose any number > 0 ! Suggested 4, 8, 16, 32, 64, 128\n",
    "\n",
    "    ################# TODO  ####################################################################\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\", ],\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # We support rank stabilized LoRA\n",
    "    loftq_config=None,  # And LoftQ\n",
    ")"
   ],
   "id": "ef532e06caa79705",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:55:53.319358Z",
     "start_time": "2025-04-06T15:55:53.308503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3.1\",  ### Use llama-3.1 template for better performance here\n",
    ")\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "    return {\"text\": texts, }"
   ],
   "id": "f0e0f53f6470bbd0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset Preperation (Loading and Refining)",
   "id": "dbc3b1c25eed263c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Filtering & Sorting",
   "id": "c103afc8ca48725f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:56:03.178477Z",
     "start_time": "2025-04-06T15:55:56.030787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_from_disk(\"dataset/ml2025-hw5/fastchat_alpaca_52k\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Add a \"text\" field to each example\n",
    "# ---------------------------\n",
    "# This function extracts the first assistant message from the conversation\n",
    "def add_text_field(example):\n",
    "    # Extract the first message where role == 'assistant'\n",
    "    assistant_texts = [msg[\"content\"] for msg in example[\"conversations\"] if msg[\"role\"] == \"assistant\"]\n",
    "    text = assistant_texts[0] if assistant_texts else \"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "# Map the function over the dataset to add the \"text\" column.\n",
    "dataset = dataset.map(add_text_field)\n",
    "\n",
    "# Print the dataset structure to confirm the new feature.\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "#################### TODO : Define a helper function for computing conversation length ###############\n",
    "# The default \"conversation length\" here refers to the length of the input (human) and output (gpt), you can modify it at your will\n",
    "def compute_conversation_length(example):\n",
    "    # Compute total word count across all messages in the 'conversations' field\n",
    "    return sum(len(message[\"content\"].split()) for message in example[\"conversations\"])\n",
    "\n",
    "\n",
    "#################### TODO ############################################################################\n",
    "# ---------------------------\n",
    "# Simple Sorting Method  (Default)\n",
    "# ---------------------------\n",
    "# Sort the dataset from longest to shortest conversation (by word count)\n",
    "sorted_dataset_simple_list = sorted(dataset, key=compute_conversation_length, reverse=True)\n",
    "\n",
    "# Convert back to a Dataset object\n",
    "sorted_dataset_simple = Dataset.from_list(sorted_dataset_simple_list)\n",
    "\n",
    "print(\"\\nTop examples sorted by simple conversation length:\")\n",
    "for entry in sorted_dataset_simple.select(range(5)):\n",
    "    print(f\"ID: {entry['id']}, Conversation Length: {compute_conversation_length(entry)}\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "############## Advanced Sorting Method (TODO : Modify the sorting key ##################\n",
    "# ---------------------------\n",
    "# Default : Sorting based on Combining conversation length with the 'score' field using a weighted sum.\n",
    "# Here, we multiply the score by 10 and add it to the conversation length.\n",
    "def advanced_sort_key(example):\n",
    "    conversation_len = compute_conversation_length(example)\n",
    "    score = example[\"score\"]\n",
    "    return 1e-5 * conversation_len + score * 1\n",
    "\n",
    "\n",
    "####################################### TODO ###########################################\n",
    "sorted_dataset_advanced_list = sorted(dataset, key=advanced_sort_key, reverse=True)\n",
    "# Convert back to a Dataset object\n",
    "sorted_dataset_advanced = Dataset.from_list(sorted_dataset_advanced_list)\n",
    "\n",
    "print(\"\\nTop examples sorted by advanced key (combination of conversation length and score):\")\n",
    "for entry in sorted_dataset_advanced.select(range(5)):\n",
    "    print(f\"ID: {entry['id']}, Advanced Key Value: {advanced_sort_key(entry)}\")"
   ],
   "id": "587b6ecf37be0495",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'conversations', 'score', 'text'],\n",
      "    num_rows: 52002\n",
      "})\n",
      "\n",
      "Top examples sorted by simple conversation length:\n",
      "ID: identity_45289, Conversation Length: 759\n",
      "ID: identity_6285, Conversation Length: 670\n",
      "ID: identity_15102, Conversation Length: 622\n",
      "ID: identity_18853, Conversation Length: 567\n",
      "ID: identity_15908, Conversation Length: 558\n",
      "\n",
      "Top examples sorted by advanced key (combination of conversation length and score):\n",
      "ID: identity_23370, Advanced Key Value: 5.00049\n",
      "ID: identity_37225, Advanced Key Value: 5.00047\n",
      "ID: identity_40362, Advanced Key Value: 5.0004\n",
      "ID: identity_8015, Advanced Key Value: 5.00037\n",
      "ID: identity_30813, Advanced Key Value: 5.00037\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Note : You are limited to use 100 sorted data among the 1000 data in the given dataset, no more than 100 data is allowed for training !!!",
   "id": "cda848724ac82727"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:56:05.474323Z",
     "start_time": "2025-04-06T15:56:05.378459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "################# TODO : select the simple or advanced dataset for training ##############\n",
    "\n",
    "dataset_used = \"sorted_dataset_advanced\"  #sorted_dataset_advanced\n",
    "\n",
    "################# TODO ###################################################################\n",
    "\n",
    "if dataset_used == \"sorted_dataset_simple\":\n",
    "    train_dataset = sorted_dataset_simple.select(\n",
    "        range(0, 100))  ### You can also select from the middle, e.g. sorted_dataset_simple.select(range(50,150))\n",
    "else:\n",
    "    train_dataset = sorted_dataset_advanced.select(range(0, 100))\n",
    "\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "train_dataset = standardize_sharegpt(train_dataset)\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)"
   ],
   "id": "c94489f5de418fe9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Standardizing format:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "346da092feb9404189d06541f33df19c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15a3ef05fac94c149ba9ea3f19f4562e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset Visualize",
   "id": "6b1558f79b0cc508"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:56:10.489604Z",
     "start_time": "2025-04-06T15:56:10.480379Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset[0][\"conversations\"]",
   "id": "22738290f81483be",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: Rewrite the sentence below into a clear and concise instruction ### Input: Please find a way to format the document',\n",
       "  'role': 'user'},\n",
       " {'content': 'Format the document.', 'role': 'assistant'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And we see how the chat template transformed these conversations.\n",
    "\n",
    "**[Notice]** Llama 3.1 Instruct's default chat template default adds `\"Cutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\"`, so do not be alarmed!"
   ],
   "id": "530cf6c6c1de9032"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:56:12.722733Z",
     "start_time": "2025-04-06T15:56:12.715752Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_dataset[0][\"text\"])",
   "id": "b16de0bbcb9cbc79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: Rewrite the sentence below into a clear and concise instruction ### Input: Please find a way to format the document<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Format the document.<|eot_id|>\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "61a7970c0034bb93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:56:18.279602Z",
     "start_time": "2025-04-06T15:56:15.957596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "################# TODO : Tweak the training hyperparameters here.  #####################\n",
    "training_config = {\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 10,\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"seed\": 3407,  ### Do not modify the seed for reproducibility\n",
    "}\n",
    "\n",
    "################# TODO #################################################################\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=training_config[\"per_device_train_batch_size\"],\n",
    "        gradient_accumulation_steps=training_config[\"gradient_accumulation_steps\"],\n",
    "        warmup_steps=training_config[\"warmup_steps\"],\n",
    "        num_train_epochs=training_config[\"num_train_epochs\"],  # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate=training_config[\"learning_rate\"],\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=training_config[\"optim\"],\n",
    "        weight_decay=training_config[\"weight_decay\"],\n",
    "        lr_scheduler_type=training_config[\"lr_scheduler_type\"],\n",
    "        seed=training_config[\"seed\"],\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",  # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ],
   "id": "b184ea70953255cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Converting train dataset to ChatML (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c84d0707cb18410aadf87a81afda2c40"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Applying chat template to train dataset (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba68e13e5f7c4d25a0518cca3c1b1445"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "581361f161d74d5a9fb31d6001e920c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Truncating train dataset (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f36c13dc70654cb8b77fac6b2eca53ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs.",
   "id": "6d97dd6f5ea26b93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:56:23.057385Z",
     "start_time": "2025-04-06T15:56:22.914475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ],
   "id": "4313636a4a96fd69",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ea9c9654a674c308d64a51d24a40ded"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer_stats = trainer.train()",
   "id": "bedfb1faba2b79"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### TODO : Curriculum Training  (Optional)\n",
    "start training the LLM with â€œeasierâ€ examples (e.g., shorter, clearer conversations) and progressively introduce more complex ones.\n",
    "\n",
    "The total data amount used to train should still not exceed 100 data."
   ],
   "id": "7ac2adad5c104291"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "############## TODO : Curriculum Training  ######################\n",
    "\n",
    "### E.g.\n",
    "### Step 1. Train on sorted_dataset_simple\n",
    "### Step 2. Train on sorted_dataset_advanced"
   ],
   "id": "6dfd6b09e4ec9f9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"Inference\"></a>\n",
    "## Inference\n"
   ],
   "id": "c0049fe6b81d995c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def parse_true_output(text):\n",
    "    \"\"\"\n",
    "    Extracts the true assistant output from the decoded model output.\n",
    "\n",
    "    It looks for the assistant header token:\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    and extracts everything after it until the first occurrence of \"<|eot_id|>\".\n",
    "    If the assistant header is not found, it falls back to the last occurrence\n",
    "    of \"<|end_header_id|>\\n\\n\". If \"<|eot_id|>\" is not found, the extraction\n",
    "    continues until the end of the string.\n",
    "    \"\"\"\n",
    "    assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    start_index = text.find(assistant_header)\n",
    "    if start_index != -1:\n",
    "        start_index += len(assistant_header)\n",
    "    else:\n",
    "        # Fallback: use the last occurrence of the generic header ending\n",
    "        generic_header = \"<|end_header_id|>\\n\\n\"\n",
    "        start_index = text.rfind(generic_header)\n",
    "        if start_index != -1:\n",
    "            start_index += len(generic_header)\n",
    "        else:\n",
    "            start_index = 0\n",
    "\n",
    "    end_index = text.find(\"<|eot_id|>\", start_index)\n",
    "    if end_index == -1:\n",
    "        end_index = len(text)\n",
    "    return text[start_index:end_index].strip()"
   ],
   "id": "e11558e2ba6fc895"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "# Load the test set JSON file (without GPT responses)\n",
    "with open(\"dataset/ml2025-hw5/test_set_evol_instruct_150.json\", \"r\") as infile:\n",
    "    test_data = json.load(infile)\n",
    "\n",
    "# Dictionary to store inference results\n",
    "inference_results = {}\n",
    "\n",
    "# Loop over each data entry in the test set\n",
    "for index, entry in enumerate(test_data):\n",
    "    entry_id = entry.get(\"id\", \"unknown_id\")\n",
    "\n",
    "    # Build the messages list from the human conversation entries\n",
    "    # (Test set is expected to have only \"human\" messages)\n",
    "    messages = []\n",
    "    for conv in entry.get(\"conversations\", []):\n",
    "        if conv.get(\"from\") == \"human\":\n",
    "            messages.append({\"role\": \"user\", \"content\": conv.get(\"value\", \"\")})\n",
    "        else:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": conv.get(\"value\", \"\")})\n",
    "\n",
    "    # Create inputs using the chat template (required for generation)\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,  # Must add for generation\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    ################# TODO : Tweak Decoding Parameters here.  #####################\n",
    "\n",
    "    # Generate model outputs\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=100,\n",
    "        use_cache=True,\n",
    "        temperature=1.5,\n",
    "        top_p=0.9,\n",
    "        top_k=30,\n",
    "    )\n",
    "\n",
    "    ################# TODO  ##########################################################\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "\n",
    "    # Parse each output to extract the true assistant response\n",
    "    parsed_outputs = [parse_true_output(output) for output in decoded_outputs]\n",
    "\n",
    "    # Store the result for the current entry\n",
    "    inference_results[entry_id] = {\n",
    "        \"input\": messages,\n",
    "        \"output\": parsed_outputs\n",
    "    }\n",
    "\n",
    "    print(f\"Inference completed for entry {entry_id}\")\n",
    "\n",
    "#Write the inference results to the prediction JSON file\n",
    "with open(f\"pred.json\", \"w\") as outfile:\n",
    "    json.dump(inference_results, outfile, indent=4)\n",
    "with open(f\"training_config.json\", \"w\") as outfile:\n",
    "    json.dump(training_config, outfile, indent=4)\n",
    "\n",
    "print(\"Inference completed for all entries in the test set.\")"
   ],
   "id": "150560f8035b44d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Saving, loading finetuned models",
   "id": "e41b0b466a54c543"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save the model",
   "id": "3fc0ddfc1de688d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ],
   "id": "e4fdf0e5a57cbaab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load the model",
   "id": "f47b58c175e91a51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"lora_model\",\n",
    "    # The folder path containing of the folder that contains adapter_model.safetensors, adapter_config.json and README.md\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference"
   ],
   "id": "3f37a58b2bb59ef6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compare the performance of the finetuned model with the original model",
   "id": "abd9e37adf61a93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the finetuned model",
   "id": "da00ef25461ce2ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:46:38.978734Z",
     "start_time": "2025-04-06T15:45:33.380008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"lora_model\",\n",
    "    # The folder path containing of the folder that contains adapter_model.safetensors, adapter_config.json and README.md\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3.1\",\n",
    ")"
   ],
   "id": "aeba43def6a70362",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.138 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:47:03.894074Z",
     "start_time": "2025-04-06T15:46:49.857576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"ä½ å¥½ï¼Œè¯·ç”¨ä¸è¶…è¿‡200å­—çš„å†…å®¹ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ã€‚\"\n",
    "messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=500,\n",
    "    use_cache=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=30,\n",
    ")\n",
    "decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "print(decoded_outputs[0])"
   ],
   "id": "4607545e4ec60e90",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ä½ å¥½ï¼Œè¯·ç”¨ä¸è¶…è¿‡200å­—çš„å†…å®¹ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ã€‚<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligence, AIï¼‰æ˜¯æŒ‡é€šè¿‡è®¡ç®—æœºæ¥æ¨¡æ‹Ÿäººç±»çš„æ™ºèƒ½ã€‚é€šè¿‡å¤§é‡çš„æ•°æ®è®­ç»ƒï¼Œäººå·¥æ™ºèƒ½å¯ä»¥è‡ªåŠ¨å­¦ä¹ å¹¶è¿è¡Œï¼Œä»¥è·å¾—ä¸äººç±»ä¸€è‡´çš„æ™ºèƒ½ã€‚äººå·¥æ™ºèƒ½å¯ä»¥åšåˆ°å„ç§å¤æ‚çš„ä»»åŠ¡ï¼Œæ¯”å¦‚å›¾ç‰‡åˆ†ç±»ã€è¯­éŸ³è¾¨è¯†ã€æ¸¸æˆç©å®¶çš„è¡Œä¸ºåˆ†æç­‰ã€‚äººå·¥æ™ºèƒ½å¯ä»¥åœ¨å®æ—¶çš„æ•æ·æ€§ã€é«˜å‡†ç¡®åº¦ã€é«˜æ•ˆç‡ã€å¯é æ€§å’Œè‡ªæˆ‘ä¿®å¤ç­‰æ–¹é¢æ¯”äººç±»æ›´å¥½ã€‚äººå·¥æ™ºèƒ½å¯ä»¥åœ¨åŒ»ç–—ã€æ•™è‚²ã€ç§‘å­¦ç ”ç©¶ã€è¯„è®ºã€è¥é”€ã€å†›äº‹ã€å·¥ç¨‹è®¾è®¡ç­‰é¢†åŸŸä½¿ç”¨ã€‚äººå·¥æ™ºèƒ½å¯ä»¥æ”¹å–„å·¥ä½œæ•ˆç‡ï¼ŒåŒæ—¶å¸®åŠ©äººç±»æ¢ç´¢æ›´å¤šçš„å¯èƒ½æ€§ã€‚äººå·¥æ™ºèƒ½çš„åº”ç”¨èŒƒå›´ä¸æ–­æ‰©å¤§ï¼Œå°†å¸¦æ¥æ›´å¤šçš„æœºå™¨äººåŒ–ã€æ··åˆç°å®ç­‰æ–°çš„æŠ€æœ¯ã€‚</s>\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the original model",
   "id": "deaa192d303dc4a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:52:28.156511Z",
     "start_time": "2025-04-06T15:51:28.592987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-2-7b-bnb-4bit\",  ### Do not change the model for any other models or quantization versions\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3.1\",\n",
    ")"
   ],
   "id": "9b3e76a2cd8e2bb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.138 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:53:41.972799Z",
     "start_time": "2025-04-06T15:53:38.958274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"ä½ å¥½ï¼Œè¯·ç”¨ä¸è¶…è¿‡200å­—çš„å†…å®¹ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ã€‚\"\n",
    "messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=500,\n",
    "    use_cache=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=30,\n",
    ")\n",
    "decoded_outputs = tokenizer.batch_decode(outputs)\n",
    "print(decoded_outputs[0])"
   ],
   "id": "68e096b00a0e00cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "ä½ å¥½ï¼Œè¯·ç”¨ä¸è¶…è¿‡200å­—çš„å†…å®¹ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ã€‚<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "æ„Ÿè°¢ä½ çš„åé¦ˆã€‚\n",
      "\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|eot_id|><|start_header_id|>system<|end_header_id|>\n",
      "</s>\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
