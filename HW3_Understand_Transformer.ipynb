{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkELbiG6ogiG"
   },
   "source": [
    "# Understanding LLM / Transformers (You cannot run the code without saving a copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvlATdaN45U8"
   },
   "source": [
    "## Check the status of your GPU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wu01corp_E0h",
    "outputId": "a1d6aa23-0ce6-4aa1-f665-c83416b23c32"
   },
   "source": [
    "!nvidia-smi"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FISzjhfg4vo6"
   },
   "source": [
    "## Installing **transformers** for further usage (please do not alter the version for stable usage of model)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N95opJpM_E0k",
    "outputId": "0ae39e03-c2fc-453b-9197-4418fbaebc66"
   },
   "source": [
    "!pip install transformers==4.47.0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLvm_l4g4O9U"
   },
   "source": [
    "## Huggingface login\n",
    "\n",
    "### You need the huggingface token (hf_token) to login to huggingface and install the gemma model. Therefore make sure you create your huggingface token. (Described in the Google slides)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sGNxSV2r_E0l"
   },
   "source": [
    "######################## TODO (Pre-requisites) ########################\n",
    "# replace `your_hf_token` with your huggingface token\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(\"token\")\n",
    "#######################################################################"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HY1e8Urn-qy8"
   },
   "source": [
    "## Download the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VefzNrAk_E0m"
   },
   "source": [
    "### Gemma Model: https://huggingface.co/google/gemma-2-2b-it\n",
    "### Please accept the lincense to download the gemma model (As described on Google Slides)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139,
     "referenced_widgets": [
      "68bce918585e40e39b1d63f85eb04392"
     ]
    },
    "id": "rd__h5Gr_E0q",
    "outputId": "9413a1fb-9b4d-457d-ddb6-0138c59397d3"
   },
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "dtype = torch.float16\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=dtype,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4cI-w4lEqvz"
   },
   "source": [
    "## Q1: Chat template Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBlbhVN4BHVG"
   },
   "source": [
    "### Evaluation Model: https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "36aa56b141e3481aae05f28c23a8050a",
      "8fac2c9c4e2a472b8236031802f2218d",
      "3b47f86f28684f4d844f4c8f6a8c1840",
      "7e5c74952e6a43b4b46138b5b923d79b",
      "204c366993e447859498fd12be791b1c",
      "0c8417db65f64928b0eafb4a920b6854",
      "1b8782da686f4ba6a9ea114842ba710a",
      "522d94c2b0534e5685215a6dc1186aff",
      "1976456eb9b94d03a7409e17d16352d9",
      "f4160dedf0864f97a62b5fe34ed62bf6",
      "ea85968ac68d40fb81db272fdb09f82a",
      "fe4bcf78454644139bdd1859eecaad57",
      "d18c6ca62715442693ee0d8f48b2ef44",
      "5632a0bb5db0475d8002e0959c68dcff",
      "f78fcd610fe34b949aca60cfc5110232",
      "3856fa26828541158e5482715315d03c",
      "415c4b954ea44e5cb865f05fe77e9793",
      "a6e981399bb04e8787e1ac53a259f4b5",
      "1dc63ecea4c7432683f027217b52b015",
      "0c165d8f30f74eb6be0f55fa4fffa463",
      "3ed628a11cd14341b59cd2647cbb16cb",
      "29ced8883bed4a189a266adc148bd4ce",
      "a616789835ac492d9e972da0b08f9fc6",
      "779f551ac47e4263ba3e57f17c46af75",
      "aded73ddb72d421d8e41bd284d733b29",
      "f09cf03862a747d38b5aa28bd77c1b0f",
      "5f16d9f6c34441ce9f593084d5014d5b",
      "13ebda78f15b4f25b3661d45df94adec",
      "0372cb36f45844aba40cf938c0f251a6",
      "c095678135c14cf2925fff178c8d98c8",
      "6cc6d70f480440588cacfbb9dd768a8a",
      "2a5f45234ec24245adfb20a5d321b281",
      "c4d16ec15a2c4223b772d2c15002a5e3",
      "92b4fa6bd01b4f31a44c86ab3e7e8d32",
      "9362d0852e474d0098dfe1f32ee13913",
      "16e9e69712904094b8168d195568d5f2",
      "6f73fdf2c53744cc81a48aa6ca408aea",
      "b0816d66cc36453ab6aa66c096e9bdd2",
      "996d3d9b49dd4ce2942f6f22aab80b27",
      "afc92038254848c792034fa4f1dd9451",
      "1de79e83531342839f21917dd5b9962f",
      "0b48fe7964e7413bb1cde1fc73ad9075",
      "53ec13b8d5d844298a13916eeafacef7",
      "a1e38eb62c0c4c6caf0ae2f36b11c873",
      "7789745ee6534ce6a122fa0c1b50e058",
      "c89abe6242674d02bf6d1adb5d32b860",
      "cb0cad112e894d84977eb97d4efb3159",
      "d3b67308c975424d8178f15b5bf860af",
      "80e7cb526d804899ae0a0822303ae738",
      "6b44757be8404bf1b8b803dde508c941",
      "574c3c72506746f791c6044750d07c36",
      "5177709ab43d43aca033036ec62fa0b5",
      "0077d825ff3347ecb41fe06155beda00",
      "d5fd17a42b324ee89b37a3131cbdca0d",
      "91c2d87b17c048aa803945c27e85a83c"
     ]
    },
    "id": "YB-nrZ-k_E0r",
    "outputId": "6268a544-06be-4c57-eb6c-bda077ced5fa"
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "SCORING_MODEL = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "SCORING_TOKENIZER = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "\n",
    "def calculate_coherence(question, answer, scoring_model=SCORING_MODEL, tokenizer=SCORING_TOKENIZER):\n",
    "  features = tokenizer([question], [answer], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "  scoring_model.eval()\n",
    "  with torch.no_grad():\n",
    "      scores = scoring_model(**features).logits.squeeze().item()\n",
    "  return scores"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JFRJMWdMrHU"
   },
   "source": [
    "### Observe whether the chat template affects the model's output results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o6uC-9nL_E0t"
   },
   "source": [
    "def generate_text_from_prompt(prompt, tokenizer, model):\n",
    "  \"\"\"\n",
    "  generate the output from the prompt.\n",
    "  param:\n",
    "    prompt (str): the prompt inputted to the model\n",
    "    tokenizer   : the tokenizer that is used to encode / decode the input / output\n",
    "    model       : the model that is used to generate the output\n",
    "\n",
    "  return:\n",
    "    the response of the model\n",
    "  \"\"\"\n",
    "  print(\"========== Prompt inputted to the model ==========\\n\", prompt)\n",
    "\n",
    "  # Tokenize the prompt\n",
    "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "  ######################## TODO (Q1.1 ~ 1.4) ########################\n",
    "  ### You can refer to https://huggingface.co/google/gemma-2-2b-it for basic usage\n",
    "  ### Make sure to use 'do_sample=False' to get a deterministic response\n",
    "  ### Otherwise the coherence score may be different from the sample answer\n",
    "\n",
    "  # Generate response\n",
    "  output_ids =model.generate(input_ids = input_ids,do_sample=False,max_new_tokens=1000)\n",
    "  ###################################################################\n",
    "  if output_ids is not None and len(output_ids) > 0:\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "  else:\n",
    "    return \"Empty Response\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eq-UV9qA_E0u",
    "outputId": "cf1cecf0-ab62-47f0-e13d-37b2eb36c611"
   },
   "source": [
    "# With chat template\n",
    "question = \"Please tell me about the key differences between supervised learning and unsupervised learning. Answer in 200 words.\"\n",
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "prompt_with_template = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "response_with_template = generate_text_from_prompt(prompt_with_template, tokenizer, model)\n",
    "\n",
    "# extract the real output from the model\n",
    "response_with_template = response_with_template.split('model\\n')[-1].strip('\\n').strip()\n",
    "\n",
    "print(\"========== Output ==========\\n\", response_with_template)\n",
    "score = calculate_coherence(question, response_with_template)\n",
    "print(f\"========== Coherence Score : {score:.4f}  ==========\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RBIhYg1D_E0v",
    "outputId": "5f9c6c5d-eebd-4929-90ea-eb00ee98c46b"
   },
   "source": [
    "# Without chat template (directly using plain text)\n",
    "response_without_template = generate_text_from_prompt(question, tokenizer, model)\n",
    "\n",
    "# extract the real output from the model\n",
    "response_without_template = response_without_template.split(question.split(' ')[-1])[-1].strip('\\n').strip()\n",
    "print(\"========== Output ==========\\n\", response_without_template)\n",
    "score = calculate_coherence(question, response_without_template)\n",
    "print(f\"========== Coherence Score : {score:.4f}  ==========\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YT8amEJSP-E"
   },
   "source": [
    "## Q2: Multi-turn conversations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hSO8qFw9_E0w",
    "outputId": "18bdeca2-fe95-4eef-b547-b8ff1da59243"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "chat_history = []\n",
    "round = 0\n",
    "print(\"Chatbot: Hello! How can I assist you today? (Type 'exit' to quit)\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    round += 1\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    chat_template_format_prompt = tokenizer.apply_chat_template(chat_history, tokenize=False, add_generation_prompt=True)\n",
    "    ######################## (Q2.1 ~ 2.3) ########################\n",
    "    # Observe the prompt with chat template format that was inputted to the model in the current round to answer Q2.1 ~ Q2.3.\n",
    "    print(f\"=== Prompt with chat template format inputted to the model on round {round} ===\\n{chat_template_format_prompt}\")\n",
    "    print(f\"===============================================\")\n",
    "    ###################################################################\n",
    "\n",
    "    inputs = tokenizer(chat_template_format_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # Get logits instead of directly generating\n",
    "    with torch.no_grad():\n",
    "        outputs_p = model(**inputs)\n",
    "\n",
    "    logits = outputs_p.logits  # Logits of the model (raw scores before softmax)\n",
    "    last_token_logits = logits[:, -1, :]  # Take the logits of the last generated token\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probs = torch.nn.functional.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "    # Get top-k tokens (e.g., 10)\n",
    "    top_k = 10\n",
    "    top_probs, top_indices = torch.topk(probs, top_k)\n",
    "\n",
    "    # Convert to numpy for plotting\n",
    "    top_probs = top_probs.cpu().squeeze().numpy()\n",
    "    top_indices = top_indices.cpu().squeeze().numpy()\n",
    "    top_tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
    "\n",
    "    # Plot probability distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=top_probs, y=top_tokens, palette=\"coolwarm\")\n",
    "    plt.xlabel(\"Probability\")\n",
    "    plt.ylabel(\"Token\")\n",
    "    plt.title(\"Top Token Probabilities for Next Word\")\n",
    "    plt.show()\n",
    "\n",
    "    # Generate response\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Chatbot: {response}\")\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": response})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oo7JKIgp0Txd"
   },
   "source": [
    "## Q3: Tokenization of Sentence"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oiBONKSS_E0x",
    "outputId": "6a21f9f8-cd53-44db-fab2-e73658f1096d"
   },
   "source": [
    "sentence = \"你好，我的名字是Jason，你呢？\" #@param {type:\"string\"}\n",
    "\n",
    "######################## TODO (Q3.1 ~ 3.4) ########################\n",
    "### You can refer to https://huggingface.co/learn/nlp-course/en/chapter2/4?fw=pt for basic tokenizer usage\n",
    "### and https://huggingface.co/docs/transformers/en/main_classes/tokenizer for full tokenizer usage\n",
    "\n",
    "\n",
    "# Encode the sentence into token IDs without adding special tokens\n",
    "token_ids = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "\n",
    "# Convert the token IDs back to their corresponding tokens (words or subwords)\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "###################################################################\n",
    "\n",
    "# Iterate through the tokens and their corresponding token IDs\n",
    "for t, t_id in zip(tokens, token_ids):\n",
    "    # Print the token and its index (ID)\n",
    "    print(f\"Token: {t}, token index: {t_id}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1ocRPtU0Txe"
   },
   "source": [
    "## Q4: Auto-regressive generation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tMF2hEH_E0y",
    "outputId": "a911892b-349f-4326-c510-a6a4cb0f2c20"
   },
   "source": [
    "from tqdm import trange\n",
    "\n",
    "max_generation_tokens = 30\n",
    "\n",
    "######################## TODO (Q4.3 ~ 4.6) ########################\n",
    "# Modify the value of k and p accordingly\n",
    "\n",
    "top_k = 10  # Set K for top-k sampling\n",
    "top_p = 0.6  # Set P for nucleus sampling\n",
    "###################################################################\n",
    "\n",
    "# Input prompt\n",
    "prompt = f\"Generate a paraphrase of the sentence 'Professor Hung-yi Lee is one of the best teachers in the domain of machine learning'. Just response with one sentence.\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "next_token_id = input_ids.input_ids.to(\"cuda\")\n",
    "attention_mask = input_ids.attention_mask.to(\"cuda\")\n",
    "cache_position = torch.arange(attention_mask.shape[1], device=\"cuda\")\n",
    "\n",
    "generated_sentences_top_k = []\n",
    "generated_sentences_top_p = []\n",
    "\n",
    "\n",
    "\n",
    "# Define the generation parameters\n",
    "generation_params = {\n",
    "    \"do_sample\": True,  # Enable sampling\n",
    "    \"max_length\": max_generation_tokens + len(input_ids.input_ids[0]),  # Total length including prompt\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,  # Ensure padding token is set\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,  # Ensure EOS token is set\n",
    "    \"bos_token_id\": tokenizer.bos_token_id,  # Ensure BOS token is set\n",
    "    \"attention_mask\": input_ids.attention_mask.to(\"cuda\"),  # Move attention mask to GPU\n",
    "    \"use_cache\": True,  # Enable caching\n",
    "    \"return_dict_in_generate\": True,  # Return generation outputs\n",
    "    \"output_scores\": False,  # Disable outputting scores\n",
    "}\n",
    "\n",
    "\n",
    "for method in [\"top-k\", \"top-p\"]:\n",
    "    for _ in trange(20):\n",
    "      if method == \"top-k\":\n",
    "        # Generate text using the model with top_k\n",
    "        generated_output = model.generate(\n",
    "            input_ids=input_ids.input_ids.to(\"cuda\"),\n",
    "            top_k=top_k,\n",
    "            **generation_params\n",
    "        )\n",
    "      elif method == \"top-p\":\n",
    "        # Generate text using the model with top_p\n",
    "        ######################## TODO (Q4.3 ~ 4.6) ########################\n",
    "        # Generate output from the model based on the input_ids and specified generation parameters\n",
    "        # You can refer to this documentation: https://huggingface.co/docs/transformers/en/main_classes/text_generation\n",
    "        # Hint: You can check how we generate the text with top_k\n",
    "\n",
    "        generated_output = model.generate(\n",
    "            input_ids=input_ids.input_ids.to(\"cuda\"),\n",
    "            top_p=top_p,\n",
    "            **generation_params\n",
    "        )\n",
    "        ###################################################################\n",
    "      else:\n",
    "        raise NotImplementedError()\n",
    "      # Decode the generated tokens\n",
    "      generated_tokens = generated_output.sequences[0, len(input_ids.input_ids[0]):]\n",
    "      decoded_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "      # Combine the prompt with the generated text\n",
    "      sentence = decoded_text.replace(\" ,\", \",\").replace(\" 's\", \"'s\").replace(\" .\", \".\").strip()\n",
    "\n",
    "      # Append the generated sentence to the appropriate list\n",
    "      if method == \"top-k\":\n",
    "          generated_sentences_top_k.append(sentence)\n",
    "      else:\n",
    "          generated_sentences_top_p.append(sentence)\n",
    "\n",
    "# Print results\n",
    "print(\"===== Top-K Sampling Output =====\")\n",
    "print()\n",
    "for idx,sentence in enumerate(generated_sentences_top_k):\n",
    "    print(f\"{idx}. {sentence}\")\n",
    "print()\n",
    "print(\"===== Top-P Sampling Output =====\")\n",
    "print()\n",
    "for idx,sentence in enumerate(generated_sentences_top_p):\n",
    "    print(f\"{idx}. {sentence}\")\n",
    "print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8RtIELr_E00",
    "outputId": "65b43c24-e148-47b0-9efd-356474423e6c"
   },
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def compute_self_bleu(generated_sentences):\n",
    "    total_bleu_score = 0\n",
    "    num_sentences = len(generated_sentences)\n",
    "\n",
    "    for i, hypothesis in enumerate(generated_sentences):\n",
    "        references = [generated_sentences[j] for j in range(num_sentences) if j != i]\n",
    "        bleu_scores = [sentence_bleu([ref.split()], hypothesis.split()) for ref in references]\n",
    "        total_bleu_score += sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "    return total_bleu_score / num_sentences\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = compute_self_bleu(generated_sentences_top_k)\n",
    "print(f\"self-BLEU Score for top_k (k={top_k}): {bleu_score:.4f}\")\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = compute_self_bleu(generated_sentences_top_p)\n",
    "print(f\"self-BLEU Score for top_p (p={top_p}): {bleu_score:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvI2LvarYe-M"
   },
   "source": [
    "## Q5: t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "ANghhPVM_E02",
    "outputId": "8b424feb-d96a-455d-a3d0-4dab2fc13a48"
   },
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "######################## (Q5.2 ~ 5.3) ########################\n",
    "# Sentences with different meanings of words\n",
    "sentences = [\n",
    "    \"I ate a fresh apple.\",  # Apple (fruit)\n",
    "    \"Apple released the new iPhone.\",  # Apple (company)\n",
    "    \"I peeled an orange and ate it.\",  # Orange (fruit)\n",
    "    \"The Orange network has great coverage.\",  # Orange (telecom)\n",
    "    \"Microsoft announced a new update.\",  # Microsoft (company)\n",
    "    \"Banana is my favorite fruit.\",  # Banana (fruit)\n",
    "]\n",
    "\n",
    "# Tokenize and move to device\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Get hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "hidden_states = outputs.hidden_states[-1]  # Extract last layer embeddings\n",
    "\n",
    "# Compute sentence-level embeddings (mean pooling)\n",
    "sentence_embeddings = hidden_states.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Words to visualize\n",
    "word_labels = [\n",
    "    \"Apple (fruit)\", \"Apple (company)\",\n",
    "    \"Orange (fruit)\", \"Orange (telecom)\",\n",
    "    \"Microsoft (company)\", \"Banana (fruit)\"\n",
    "]\n",
    "\n",
    "# Reduce to 2D using t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(sentence_embeddings)\n",
    "\n",
    "# Plot the embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = [\"red\", \"blue\", \"orange\", \"purple\", \"green\", \"brown\"]\n",
    "for i, label in enumerate(word_labels):\n",
    "    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], color=colors[i], s=100)\n",
    "    plt.text(embeddings_2d[i, 0] + 0.1, embeddings_2d[i, 1] + 0.1, label, fontsize=12, color=colors[i])\n",
    "\n",
    "plt.xlabel(\"t-SNE Dim 1\")\n",
    "plt.ylabel(\"t-SNE Dim 2\")\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings\")\n",
    "plt.show()\n",
    "##################################################"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYDqomF2YWyX"
   },
   "source": [
    "## Q6: Observe the Attention Weight"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767
    },
    "id": "cmqWxTNM_E03",
    "outputId": "bc0436b6-454a-4e08-aee9-371a01bc399f"
   },
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Input prompt for text generation\n",
    "prompt = \"Google is\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")  # Tokenize the input prompt\n",
    "next_token_id = input_ids.input_ids.to(\"cuda\")  # Move input token ids to GPU\n",
    "attention_mask = input_ids.attention_mask.to(\"cuda\")  # Move attention mask to GPU\n",
    "cache_position = torch.arange(attention_mask.shape[1], device=\"cuda\")  # Position for the KV cache\n",
    "\n",
    "# Set the number of tokens to generate and other parameters\n",
    "generation_tokens = 15  # Limit for visualization (number of tokens to generate)\n",
    "total_tokens = generation_tokens + next_token_id.size(1) - 1  # Total tokens to handle\n",
    "layer_idx = 10  # Specify the layer index for attention visualization\n",
    "head_idx = 7  # Specify the attention head index to visualize\n",
    "\n",
    "# KV cache setup for caching key/values across time steps\n",
    "from transformers.cache_utils import HybridCache\n",
    "kv_cache = HybridCache(config=model.config, max_batch_size=1, max_cache_len=total_tokens, device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "generated_tokens = []  # List to store generated tokens\n",
    "attentions = None  # Placeholder to store attention weights\n",
    "\n",
    "num_new_tokens = 0  # Counter for the number of new tokens generated\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Generate tokens and collect attention weights for visualization\n",
    "for num_new_tokens in range(generation_tokens):\n",
    "    with torch.no_grad():  # Disable gradients during inference for efficiency\n",
    "        # Pass the input through the model to get the next token prediction and attention weights\n",
    "        outputs = model(\n",
    "            next_token_id,\n",
    "            attention_mask=attention_mask,\n",
    "            cache_position=cache_position,\n",
    "            use_cache=True,  # Use the KV cache for efficiency\n",
    "            past_key_values=kv_cache,  # Provide the cached key-value pairs for fast inference\n",
    "            output_attentions=True  # Enable the extraction of attention weights\n",
    "        )\n",
    "    ######################## TODO (Q6.1 ~ 6.4) ########################\n",
    "    ### You can refer to https://huggingface.co/docs/transformers/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput.attentions to see the structure of model output attentions\n",
    "    # Get the logits for the last generated token from outputs\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "    # Extract the attention scores from the model's outputs\n",
    "    attention_scores = outputs.attentions\n",
    "    ###################################################################\n",
    "\n",
    "    # Extract attention weights for the specified layer and head\n",
    "    last_layer_attention = attention_scores[layer_idx][0][head_idx].detach().cpu().numpy()\n",
    "\n",
    "    # If it's the first generated token, initialize the attentions array\n",
    "    if num_new_tokens == 0:\n",
    "        attentions = last_layer_attention\n",
    "    else:\n",
    "        # Append the current attention weights to the existing array\n",
    "        attentions = np.append(attentions, last_layer_attention, axis=0)\n",
    "\n",
    "    # Choose the next token to generate based on the highest probability (logits)\n",
    "    next_token_id = logits.argmax(dim=-1)\n",
    "    generated_tokens.append(next_token_id.item())  # Add the token ID to the generated tokens list\n",
    "\n",
    "    # Update the attention mask and next token ID for the next iteration\n",
    "    attention_mask = torch.cat([attention_mask, torch.ones(1, 1, device=\"cuda\")], dim=-1)  # Add a new attention mask for the generated token\n",
    "    next_token_id = next_token_id.unsqueeze(0)  # Convert the token ID to the required shape\n",
    "\n",
    "    # Update the KV cache with the new past key-values\n",
    "    kv_cache = outputs.past_key_values\n",
    "    cache_position = cache_position[-1:] + 1  # Update the cache position for the next iteration\n",
    "\n",
    "# Decode the generated tokens into human-readable text\n",
    "generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "full_text = prompt + generated_text  # Combine the prompt with the generated text\n",
    "\n",
    "# Tokenize all the generated text (prompt + generated)\n",
    "tokens = tokenizer.tokenize(full_text)\n",
    "\n",
    "# Function to plot a heatmap of attention weights\n",
    "def plot_attention(attn_matrix, tokens, title=\"Attention Heatmap\"):\n",
    "    plt.figure(figsize=(10, 8))  # Set the figure size\n",
    "    sns.heatmap(attn_matrix, xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\", annot=False)  # Plot the attention matrix as a heatmap\n",
    "    plt.xlabel(\"Key Tokens\")\n",
    "    plt.ylabel(\"Query Tokens\")\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "    plt.yticks(rotation=0)  # Rotate y-axis labels\n",
    "    plt.show()\n",
    "\n",
    "# Plot the attention heatmap for the last generated token\n",
    "plot_attention(attentions, tokens, title=f\"Attention Weights for Generated Token of Layer {layer_idx}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz2z5Q4TYlyE"
   },
   "source": [
    "## Q7: Observe the Activation Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2li0_FBwbit"
   },
   "source": [
    "The following code is referred from official Gemma tutorials: [Gemma Tutorial From Scratch](https://colab.research.google.com/drive/17dQFYUYnuKnP6OwQPH9v_GSYUW5aj-Rp#scrollTo=2-i7YRVLgKoT) and [SAELens](https://github.com/jbloomAus/SAELens/blob/main/tutorials/tutorial_2_0.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7pzghoK7_E06"
   },
   "source": [
    "!pip install sae-lens"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R-fFoznQ_E07",
    "outputId": "6563cae5-68ee-4546-ff09-436258de0203"
   },
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res-canonical\",\n",
    "    sae_id = \"layer_20/width_16k/canonical\",\n",
    ")\n",
    "\n",
    "print(sae, cfg_dict, sparsity)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "Lr7fEFGb_E07",
    "outputId": "3f80785e-aed6-486e-b538-1e4ba4c2e476"
   },
   "source": [
    "from IPython.display import IFrame\n",
    "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "def get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"20-gemmascope-res-16k\", feature_idx=0):\n",
    "    return html_template.format(sae_release, sae_id, feature_idx)\n",
    "\n",
    "########################## TODO (Q7.1) ############################\n",
    "html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"20-gemmascope-res-16k\", feature_idx=10004)\n",
    "IFrame(html, width=1200, height=600)\n",
    "###################################################################"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51VLXHjtGRLN"
   },
   "source": [
    "## Q7.2~7.3: Maximum activations comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 983
    },
    "id": "UVtTuMOS_E09",
    "outputId": "a0440e22-b300-4242-9216-881b85fe7797"
   },
   "source": [
    "######################## (Q7.2 ~ 7.3) ########################\n",
    "\n",
    "def get_max_activation(model, tokenizer, sae, prompt, feature_idx=10004):\n",
    "    \"\"\"\n",
    "    Computes the maximum activation of a specific feature in a Sparse Autoencoder (SAE)\n",
    "    for a given prompt.\n",
    "\n",
    "    Args:\n",
    "        model: The Transformer model used for generating hidden states.\n",
    "        tokenizer: The tokenizer for encoding the prompt.\n",
    "        sae: The Sparse Autoencoder for encoding hidden states.\n",
    "        prompt (str): The input text prompt.\n",
    "        feature_idx (int, optional): The index of the feature in SAE. Defaults to 10004.\n",
    "\n",
    "    Returns:\n",
    "        float: The maximum activation value for the specified feature index.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    sae.to(device)\n",
    "\n",
    "    # Tokenize the input prompt and get model outputs\n",
    "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model(tokens, output_hidden_states=True)\n",
    "\n",
    "    # Extract hidden states from the specified layer\n",
    "    hidden_states = outputs.hidden_states[sae.cfg.hook_layer]\n",
    "\n",
    "    # Encode hidden states using SAE\n",
    "    sae_in = hidden_states\n",
    "    feature_acts = sae.encode(sae_in).squeeze()  # Shape: (batch_size * seq_len, num_features)\n",
    "    feature_acts = feature_acts.reshape(-1, feature_acts.shape[-1])\n",
    "\n",
    "    # Compute max activation for the specified feature index\n",
    "    max_activation = -float(\"inf\")\n",
    "    batch_max_activation = feature_acts[:, feature_idx].max().item()\n",
    "    max_activation = max(max_activation, batch_max_activation)\n",
    "\n",
    "    # Plot activation distribution\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(feature_acts[:, feature_idx].cpu().detach().numpy(), bins=50, alpha=0.75, color='blue', edgecolor='black')\n",
    "    plt.xlabel(f\"Activation values (Feature {feature_idx})\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Activation Distribution for Feature {feature_idx} - Prompt: '{prompt}'\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return max_activation\n",
    "\n",
    "feature_idx = 10004\n",
    "# Define the prompts\n",
    "prompt_a = \"Time travel offers me the opportunity to correct past errors, but it comes with its own set of risks.\"\n",
    "prompt_b = \"I accept that my decisions shape my future, and though mistakes are inevitable, they define who I become.\"\n",
    "\n",
    "# Calculate the maximum activations for each prompt using the feature index\n",
    "max_activation_a = get_max_activation(model, tokenizer, sae, prompt_a, feature_idx=feature_idx)\n",
    "max_activation_b = get_max_activation(model, tokenizer, sae, prompt_b, feature_idx=feature_idx)\n",
    "\n",
    "# Print the comparison\n",
    "print(f\"max_activation for prompt_a: {max_activation_a}\")\n",
    "print(f\"max_activation for prompt_b: {max_activation_b}\")\n",
    "###########################################################"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFg9hOKzGW8y"
   },
   "source": [
    "## Q7.4~7.6: Activation distribution for specific layer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YpzyjeYo_E0-"
   },
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_token_activations(model, tokenizer, sae, prompt, feature_idx=10004, layer_idx=0):\n",
    "    \"\"\"\n",
    "    Plots activations for each token in a specific layer.\n",
    "\n",
    "    Args:\n",
    "        model: The transformer model.\n",
    "        tokenizer: Tokenizer for encoding input text.\n",
    "        sae: Sparse Autoencoder model.\n",
    "        prompt: Input text string.\n",
    "        feature_idx: Index of the feature to analyze.\n",
    "        layer_idx: Layer to analyze (None uses sae.cfg.hook_layer).\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    sae.to(device)\n",
    "\n",
    "    # Tokenize input and get model output\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    token_ids = tokens[\"input_ids\"].to(device)\n",
    "    token_list = tokenizer.convert_ids_to_tokens(token_ids.squeeze().tolist())\n",
    "\n",
    "    outputs = model(token_ids, output_hidden_states=True)\n",
    "\n",
    "    # Choose layer\n",
    "    layer_idx = layer_idx if layer_idx is not None else sae.cfg.hook_layer\n",
    "    hidden_states = outputs.hidden_states[layer_idx]\n",
    "\n",
    "    # Pass through SAE\n",
    "    sae_in = hidden_states\n",
    "    feature_acts = sae.encode(sae_in).squeeze()  # (batch_size, seq_len, num_features)\n",
    "    print(f\"feature_acts shape: {feature_acts.shape}\")\n",
    "\n",
    "    # Extract activations for the chosen feature\n",
    "    activations = feature_acts[:, feature_idx].squeeze().cpu().detach().numpy()\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(len(token_list)), activations, color='blue', alpha=0.7)\n",
    "    plt.xticks(range(len(token_list)), token_list, rotation=45)\n",
    "    plt.xlabel(\"Tokens\")\n",
    "    plt.ylabel(f\"Activation Value (Feature {feature_idx})\")\n",
    "    plt.title(f\"Token-wise Activations for Layer {layer_idx}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "######################## (Q7.4 ~ 7.6) ########################\n",
    "# Simply observe the figure\n",
    "layer_idx = 24\n",
    "prompt = \"Time travel will become a reality as technology continues to advance.\"\n",
    "plot_token_activations(model, tokenizer, sae, prompt_a, feature_idx, layer_idx)\n",
    "###################################################################"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwLP2sXZGcrb"
   },
   "source": [
    "## Q7.7~7.9: Activation distribution for specific token"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gTyWXsOx_E1A"
   },
   "source": [
    "def plot_layer_activations(model, tokenizer, sae, prompt, token_idx=0, feature_idx=10004):\n",
    "    \"\"\"\n",
    "    Plots activations of a specific token across all layers.\n",
    "\n",
    "    Args:\n",
    "        model: The transformer model.\n",
    "        tokenizer: Tokenizer for encoding input text.\n",
    "        sae: Sparse Autoencoder model.\n",
    "        prompt: Input text string.\n",
    "        token_idx: Index of the token to analyze.\n",
    "        feature_idx: Index of the feature to analyze.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    sae.to(device)\n",
    "\n",
    "    # Tokenize input and get model output\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    token_ids = tokens[\"input_ids\"].to(device)\n",
    "    token_list = tokenizer.convert_ids_to_tokens(token_ids.squeeze().tolist())\n",
    "\n",
    "    outputs = model(token_ids, output_hidden_states=True)\n",
    "\n",
    "    # Collect activations across all layers\n",
    "    num_layers = len(outputs.hidden_states)\n",
    "    activations = []\n",
    "\n",
    "    for layer_idx in range(num_layers):\n",
    "        hidden_states = outputs.hidden_states[layer_idx]\n",
    "        sae_in = hidden_states\n",
    "        feature_acts = sae.encode(sae_in).squeeze()  # (batch_size, seq_len, num_features)\n",
    "        # print(f\"feature_acts shape: {feature_acts.shape}\")\n",
    "        activations.append(feature_acts[token_idx, feature_idx].item())\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(num_layers), activations, marker=\"o\", linestyle=\"-\", color=\"blue\")\n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(f\"Activation Value (Feature {feature_idx})\")\n",
    "    plt.title(f\"Activation Across Layers for Token '{token_list[token_idx]}'\")\n",
    "    plt.xticks(range(num_layers))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "######################## (Q7.7 ~ 7.9) ########################\n",
    "# Alter the token index to observe the figure\n",
    "token_idx = 1\n",
    "prompt = \"Time travel will become a reality as technology continues to advance.\"\n",
    "plot_layer_activations(model, tokenizer, sae, prompt, token_idx)\n",
    "###################################################################"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0077d825ff3347ecb41fe06155beda00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0372cb36f45844aba40cf938c0f251a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0b48fe7964e7413bb1cde1fc73ad9075": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0c165d8f30f74eb6be0f55fa4fffa463": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0c8417db65f64928b0eafb4a920b6854": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13ebda78f15b4f25b3661d45df94adec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16e9e69712904094b8168d195568d5f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1de79e83531342839f21917dd5b9962f",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0b48fe7964e7413bb1cde1fc73ad9075",
      "value": 231508
     }
    },
    "1976456eb9b94d03a7409e17d16352d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1b8782da686f4ba6a9ea114842ba710a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1dc63ecea4c7432683f027217b52b015": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1de79e83531342839f21917dd5b9962f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "204c366993e447859498fd12be791b1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29ced8883bed4a189a266adc148bd4ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a5f45234ec24245adfb20a5d321b281": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36aa56b141e3481aae05f28c23a8050a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8fac2c9c4e2a472b8236031802f2218d",
       "IPY_MODEL_3b47f86f28684f4d844f4c8f6a8c1840",
       "IPY_MODEL_7e5c74952e6a43b4b46138b5b923d79b"
      ],
      "layout": "IPY_MODEL_204c366993e447859498fd12be791b1c"
     }
    },
    "3856fa26828541158e5482715315d03c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b47f86f28684f4d844f4c8f6a8c1840": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_522d94c2b0534e5685215a6dc1186aff",
      "max": 794,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1976456eb9b94d03a7409e17d16352d9",
      "value": 794
     }
    },
    "3ed628a11cd14341b59cd2647cbb16cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "415c4b954ea44e5cb865f05fe77e9793": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5177709ab43d43aca033036ec62fa0b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "522d94c2b0534e5685215a6dc1186aff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53ec13b8d5d844298a13916eeafacef7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5632a0bb5db0475d8002e0959c68dcff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1dc63ecea4c7432683f027217b52b015",
      "max": 90870598,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0c165d8f30f74eb6be0f55fa4fffa463",
      "value": 90870598
     }
    },
    "574c3c72506746f791c6044750d07c36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5f16d9f6c34441ce9f593084d5014d5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b44757be8404bf1b8b803dde508c941": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6cc6d70f480440588cacfbb9dd768a8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6f73fdf2c53744cc81a48aa6ca408aea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_53ec13b8d5d844298a13916eeafacef7",
      "placeholder": "​",
      "style": "IPY_MODEL_a1e38eb62c0c4c6caf0ae2f36b11c873",
      "value": " 232k/232k [00:00&lt;00:00, 7.76MB/s]"
     }
    },
    "7789745ee6534ce6a122fa0c1b50e058": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c89abe6242674d02bf6d1adb5d32b860",
       "IPY_MODEL_cb0cad112e894d84977eb97d4efb3159",
       "IPY_MODEL_d3b67308c975424d8178f15b5bf860af"
      ],
      "layout": "IPY_MODEL_80e7cb526d804899ae0a0822303ae738"
     }
    },
    "779f551ac47e4263ba3e57f17c46af75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13ebda78f15b4f25b3661d45df94adec",
      "placeholder": "​",
      "style": "IPY_MODEL_0372cb36f45844aba40cf938c0f251a6",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "7e5c74952e6a43b4b46138b5b923d79b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4160dedf0864f97a62b5fe34ed62bf6",
      "placeholder": "​",
      "style": "IPY_MODEL_ea85968ac68d40fb81db272fdb09f82a",
      "value": " 794/794 [00:00&lt;00:00, 55.6kB/s]"
     }
    },
    "80e7cb526d804899ae0a0822303ae738": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8fac2c9c4e2a472b8236031802f2218d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c8417db65f64928b0eafb4a920b6854",
      "placeholder": "​",
      "style": "IPY_MODEL_1b8782da686f4ba6a9ea114842ba710a",
      "value": "config.json: 100%"
     }
    },
    "91c2d87b17c048aa803945c27e85a83c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "92b4fa6bd01b4f31a44c86ab3e7e8d32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9362d0852e474d0098dfe1f32ee13913",
       "IPY_MODEL_16e9e69712904094b8168d195568d5f2",
       "IPY_MODEL_6f73fdf2c53744cc81a48aa6ca408aea"
      ],
      "layout": "IPY_MODEL_b0816d66cc36453ab6aa66c096e9bdd2"
     }
    },
    "9362d0852e474d0098dfe1f32ee13913": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_996d3d9b49dd4ce2942f6f22aab80b27",
      "placeholder": "​",
      "style": "IPY_MODEL_afc92038254848c792034fa4f1dd9451",
      "value": "vocab.txt: 100%"
     }
    },
    "996d3d9b49dd4ce2942f6f22aab80b27": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1e38eb62c0c4c6caf0ae2f36b11c873": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a616789835ac492d9e972da0b08f9fc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_779f551ac47e4263ba3e57f17c46af75",
       "IPY_MODEL_aded73ddb72d421d8e41bd284d733b29",
       "IPY_MODEL_f09cf03862a747d38b5aa28bd77c1b0f"
      ],
      "layout": "IPY_MODEL_5f16d9f6c34441ce9f593084d5014d5b"
     }
    },
    "a6e981399bb04e8787e1ac53a259f4b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aded73ddb72d421d8e41bd284d733b29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c095678135c14cf2925fff178c8d98c8",
      "max": 316,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6cc6d70f480440588cacfbb9dd768a8a",
      "value": 316
     }
    },
    "afc92038254848c792034fa4f1dd9451": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0816d66cc36453ab6aa66c096e9bdd2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c095678135c14cf2925fff178c8d98c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4d16ec15a2c4223b772d2c15002a5e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c89abe6242674d02bf6d1adb5d32b860": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b44757be8404bf1b8b803dde508c941",
      "placeholder": "​",
      "style": "IPY_MODEL_574c3c72506746f791c6044750d07c36",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "cb0cad112e894d84977eb97d4efb3159": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5177709ab43d43aca033036ec62fa0b5",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0077d825ff3347ecb41fe06155beda00",
      "value": 112
     }
    },
    "d18c6ca62715442693ee0d8f48b2ef44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_415c4b954ea44e5cb865f05fe77e9793",
      "placeholder": "​",
      "style": "IPY_MODEL_a6e981399bb04e8787e1ac53a259f4b5",
      "value": "model.safetensors: 100%"
     }
    },
    "d3b67308c975424d8178f15b5bf860af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5fd17a42b324ee89b37a3131cbdca0d",
      "placeholder": "​",
      "style": "IPY_MODEL_91c2d87b17c048aa803945c27e85a83c",
      "value": " 112/112 [00:00&lt;00:00, 11.0kB/s]"
     }
    },
    "d5fd17a42b324ee89b37a3131cbdca0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea85968ac68d40fb81db272fdb09f82a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f09cf03862a747d38b5aa28bd77c1b0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a5f45234ec24245adfb20a5d321b281",
      "placeholder": "​",
      "style": "IPY_MODEL_c4d16ec15a2c4223b772d2c15002a5e3",
      "value": " 316/316 [00:00&lt;00:00, 19.5kB/s]"
     }
    },
    "f4160dedf0864f97a62b5fe34ed62bf6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f78fcd610fe34b949aca60cfc5110232": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ed628a11cd14341b59cd2647cbb16cb",
      "placeholder": "​",
      "style": "IPY_MODEL_29ced8883bed4a189a266adc148bd4ce",
      "value": " 90.9M/90.9M [00:00&lt;00:00, 186MB/s]"
     }
    },
    "fe4bcf78454644139bdd1859eecaad57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d18c6ca62715442693ee0d8f48b2ef44",
       "IPY_MODEL_5632a0bb5db0475d8002e0959c68dcff",
       "IPY_MODEL_f78fcd610fe34b949aca60cfc5110232"
      ],
      "layout": "IPY_MODEL_3856fa26828541158e5482715315d03c"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
